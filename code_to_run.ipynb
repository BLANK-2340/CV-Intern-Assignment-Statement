{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9cYbvWp5z_Q5",
        "outputId": "60e2264c-cfbd-4ace-f06d-5d02a832f0f8"
      },
      "outputs": [],
      "source": [
        "!pip install addict\n",
        "!pip install yapf\n",
        "!pip install timm\n",
        "!pip install pycocotools matplotlib tqdm\n",
        "from collections.abc import Iterable\n",
        "!pip install tqdm matplotlib\n",
        "import sys\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import torchvision.transforms as T\n",
        "import importlib.util\n",
        "from util.slconfig import SLConfig\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import json\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "F3w0E_ZmVoqE",
        "outputId": "caba62a2-625c-4ee0-9520-d936f38de944"
      },
      "outputs": [],
      "source": [
        "# Change to the correct directory\n",
        "%cd \"/DINO/models/dino/ops/\"\n",
        "\n",
        "# Run the setup script to compile the CUDA extensions\n",
        "!python setup.py build install\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HkidTe6yCi2",
        "outputId": "9b0cd94b-be4c-4b1b-9a68-251165db9791"
      },
      "outputs": [],
      "source": [
        "\n",
        "sys.path.append('/DINO')  # Correct path to your DINO repo\n",
        "\n",
        "\n",
        "# Manually import the modified coco.py from the specified path\n",
        "spec = importlib.util.spec_from_file_location(\"coco\", \"/DINO/datasets/coco.py\")   # Correct path to your coco.py file\n",
        "\n",
        "coco = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(coco)\n",
        "\n",
        "# Argument class to simulate the input to the build function\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.coco_path = \"  place holder \"  # Path to your dataset folder\n",
        "        self.modelname = 'dino' \n",
        "        self.fix_size = False  \n",
        "        self.masks = False  \n",
        "\n",
        "\n",
        "# Custom collate function to handle variable image sizes\n",
        "def collate_fn(batch):\n",
        "    images, targets = zip(*batch)\n",
        "    # Pad images to the same size\n",
        "    images = [T.functional.resize(image, (800, 1333)) for image in images] \n",
        "    images = torch.stack(images, dim=0) \n",
        "    return images, targets\n",
        "\n",
        "\n",
        "# Initialize the dataset using the build function\n",
        "args = Args()\n",
        "image_set = \"train\"  # You can switch between \"train\" and \"val\" to test both\n",
        "dataset = coco.build(image_set, args)\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "for images, targets in data_loader:\n",
        "    print(f\"Loaded {len(images)} images and {len(targets)} annotations\")\n",
        "    print(f\"Image sizes: {[img.size() for img in images]}\")\n",
        "    print(f\"Annotations: {targets}\")\n",
        "    break  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Gf15GYvTdu2_",
        "outputId": "4a62cbe1-6429-4602-ca31-32559f36a53e"
      },
      "outputs": [],
      "source": [
        "!ls \"DINO/models/dino/ops/build/lib.linux-x86_64-3.*/MultiScaleDeformableAttention*\"\n",
        "# Navigate to the ops directory\n",
        "%cd \"DINO/models/dino/ops/\"\n",
        "\n",
        "# Capture the log of the build process\n",
        "!python setup.py build install > build_log.txt 2>&1\n",
        "\n",
        "# Display the last 50 lines of the build log\n",
        "!tail -n 50 build_log.txt\n",
        "\n",
        "!pip list\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oabLo2Jvh9MN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Adding paths to the compiled MultiScaleDeformableAttention module\n",
        "sys.path.append('/usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg')\n",
        "sys.path.append('DINO/models/dino/ops/build/lib.linux-x86_64-cpython-310/')\n",
        "\n",
        "# Try importing the module again\n",
        "import MultiScaleDeformableAttention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PEcurpCHkCyp",
        "outputId": "e476b6e3-1181-498d-b946-0f3efb6f0fea"
      },
      "outputs": [],
      "source": [
        "\n",
        "for key, value in vars(args).items():\n",
        "    print(f\"{key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Pz7fidPCWtk",
        "outputId": "c43e41a4-0de7-4921-d897-3216184c7eee"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def check_config(config_file):\n",
        "    args = SLConfig.fromfile(config_file)\n",
        "\n",
        "    required_params = [\n",
        "        'dataset_file', 'coco_path', 'num_classes', 'lr', 'batch_size',\n",
        "        'weight_decay', 'epochs', 'lr_drop', 'modelname', 'backbone',\n",
        "        'num_queries', 'aux_loss', 'set_cost_class', 'set_cost_bbox',\n",
        "        'set_cost_giou', 'mask_loss_coef', 'dice_loss_coef', 'bbox_loss_coef',\n",
        "        'giou_loss_coef'\n",
        "    ]\n",
        "\n",
        "    missing_params = []\n",
        "    for param in required_params:\n",
        "        if not hasattr(args, param):\n",
        "            missing_params.append(param)\n",
        "\n",
        "    if missing_params:\n",
        "        print(f\"Missing parameters in {config_file}:\")\n",
        "        for param in missing_params:\n",
        "            print(f\"- {param}\")\n",
        "    else:\n",
        "        print(f\"All required parameters are present in {config_file}\")\n",
        "\n",
        "    # Check if coco_path exists\n",
        "    if hasattr(args, 'coco_path'):\n",
        "        if not os.path.exists(args.coco_path):\n",
        "            print(f\"Warning: coco_path '{args.coco_path}' does not exist\")\n",
        "        else:\n",
        "            print(f\"coco_path '{args.coco_path}' exists\")\n",
        "\n",
        "    # Print all parameters for reference\n",
        "    print(\"\\nAll parameters:\")\n",
        "    for key, value in vars(args).items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config_file = 'DINO/config/DINO/DINO_4scale.py'\n",
        "    check_config(config_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3C81vBlXNU9",
        "outputId": "98e40dc2-41e5-432a-b385-cb82c24df358"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Update the __init__.py file\n",
        "init_content = \"\"\"\n",
        "from .dino.dino import build_dino\n",
        "\n",
        "print(\"Debug: __init__.py loaded\")\n",
        "\n",
        "def build_model(args):\n",
        "    print(\"Debug: build_model function called from __init__.py\")\n",
        "    return build_dino(args)\n",
        "\n",
        "print(\"Debug: build_model function defined in __init__.py\")\n",
        "\"\"\"\n",
        "\n",
        "init_path = 'DINO/models/__init__.py'\n",
        "with open(init_path, 'w') as f:\n",
        "    f.write(init_content)\n",
        "\n",
        "print(f\"Updated {init_path}\")\n",
        "\n",
        "\n",
        "import importlib\n",
        "import models\n",
        "importlib.reload(models)\n",
        "\n",
        "\n",
        "sys.path.append('DINO')\n",
        "\n",
        "print(\"Debug: Current working directory:\", os.getcwd())\n",
        "print(\"Debug: Content of DINO directory:\")\n",
        "print(os.listdir('DINO'))\n",
        "print(\"Debug: Content of models directory:\")\n",
        "print(os.listdir('DINO/models'))\n",
        "\n",
        "print(\"Debug: Before imports\")\n",
        "from util.slconfig import SLConfig\n",
        "print(\"Debug: After SLConfig import\")\n",
        "from models import build_model\n",
        "print(\"Debug: After build_model import\")\n",
        "from models.dino.dino import build_dino\n",
        "print(\"Debug: After build_dino import\")\n",
        "\n",
        "def test_build_model():\n",
        "    print(\"Testing DINO model build...\")\n",
        "\n",
        "    # Load configuration\n",
        "    config_file = 'DINO/config/DINO/DINO_4scale.py'\n",
        "    args = SLConfig.fromfile(config_file)\n",
        "\n",
        "    # Add necessary arguments if they're not in the config file\n",
        "    if not hasattr(args, 'num_classes'):\n",
        "        args.num_classes = 91\n",
        "    if not hasattr(args, 'num_queries'):\n",
        "        args.num_queries = 900\n",
        "    if not hasattr(args, 'aux_loss'):\n",
        "        args.aux_loss = True\n",
        "    if not hasattr(args, 'with_box_refine'):\n",
        "        args.with_box_refine = False\n",
        "    if not hasattr(args, 'two_stage'):\n",
        "        args.two_stage = False\n",
        "    if not hasattr(args, 'device'):\n",
        "        args.device = 'cuda'\n",
        "\n",
        "    print(\"\\nChecking build_dino import:\")\n",
        "    print(f\"build_dino in models.dino.dino: {hasattr(build_dino, '__call__')}\")\n",
        "\n",
        "    try:\n",
        "        print(\"\\nTrying to build model using build_model:\")\n",
        "        model, criterion, postprocessors = build_model(args)\n",
        "        print(\"Model built successfully using build_model!\")\n",
        "        print(f\"Model type: {type(model)}\")\n",
        "        print(f\"Criterion type: {type(criterion)}\")\n",
        "        print(f\"Postprocessors type: {type(postprocessors)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while building the model using build_model: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    try:\n",
        "        print(\"\\nTrying to build model using build_dino directly:\")\n",
        "        model, criterion, postprocessors = build_dino(args)\n",
        "        print(\"Model built successfully using build_dino directly!\")\n",
        "        print(f\"Model type: {type(model)}\")\n",
        "        print(f\"Criterion type: {type(criterion)}\")\n",
        "        print(f\"Postprocessors type: {type(postprocessors)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while building the model using build_dino directly: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_build_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWWH1t2pESpg",
        "outputId": "d4f21103-0fe1-4806-ec1d-bb8f96b5d178"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install pycocotools\n",
        "\n",
        "import sys\n",
        "sys.path.append('DINO')\n",
        "\n",
        "from util.slconfig import SLConfig\n",
        "from datasets import build_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def build_and_check_dataset():\n",
        "    config_file = 'DINO/config/DINO/DINO_4scale.py'\n",
        "    args = SLConfig.fromfile(config_file)\n",
        "\n",
        "    try:\n",
        "        dataset_val = build_dataset(image_set='val', args=args)\n",
        "        data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, num_workers=args.num_workers)\n",
        "\n",
        "\n",
        "        print(f\"Dataset length: {len(dataset_val)}\")\n",
        "        print(f\"Dataset type: {type(dataset_val)}\")\n",
        "\n",
        "        # Check the first item in the dataset\n",
        "        first_item = next(iter(data_loader_val))\n",
        "        print(\"\\nFirst item in the dataset:\")\n",
        "        print(f\"Images shape: {first_item[0].shape}\")\n",
        "        print(f\"Targets type: {type(first_item[1])}\")\n",
        "        print(\"Targets content:\")\n",
        "        print(first_item[1])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "\n",
        "build_and_check_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oX9yXyhyXYSj",
        "outputId": "afefbaa2-c171-4869-abde-d97259402094"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Add DINO directory to sys.path\n",
        "sys.path.append('DINO')\n",
        "\n",
        "from util.slconfig import SLConfig\n",
        "from models import build_model\n",
        "from datasets import build_dataset\n",
        "from util.misc import nested_tensor_from_tensor_list\n",
        "from util.box_ops import box_cxcywh_to_xyxy\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "\n",
        "def load_model(args):\n",
        "    model, _, postprocessors = build_model(args)\n",
        "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    model.eval()\n",
        "    return model, postprocessors\n",
        "\n",
        "def process_image(image_path, model, postprocessors, device):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    w, h = image.size\n",
        "    img = torch.from_numpy(np.array(image)).permute(2, 0, 1).float()\n",
        "    img = nested_tensor_from_tensor_list([img]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "\n",
        "    results = postprocessors['bbox'](outputs, torch.tensor([[w, h]]).to(device))[0]\n",
        "    return results\n",
        "\n",
        "def visualize_detections(image, boxes, labels, scores, output_path):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.imshow(image)\n",
        "    ax = plt.gca()\n",
        "\n",
        "    for box, label, score in zip(boxes, labels, scores):\n",
        "        x1, y1, x2, y2 = box\n",
        "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='red', linewidth=2)\n",
        "        ax.add_patch(rect)\n",
        "        plt.text(x1, y1, f'{label}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n",
        "    plt.close()\n",
        "\n",
        "def main():\n",
        "    # Load configuration\n",
        "    config_file = 'DINO/config/DINO/DINO_4scale.py'\n",
        "    args = SLConfig.fromfile(config_file)\n",
        "    args.device = 'cuda'\n",
        "    args.resume = 'DINO-4scale model/checkpoint0033_4scale.pth'\n",
        "\n",
        "    # Create output directory\n",
        "    output_dir = 'output path'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Load model\n",
        "    model, postprocessors = load_model(args)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Load validation dataset\n",
        "    dataset_val = build_dataset(image_set='val', args=args)\n",
        "\n",
        "    # Initialize COCO API\n",
        "    ann_file = os.path.join(args.coco_path, 'val', 'val.json')\n",
        "    coco_gt = COCO(ann_file)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for img_id in tqdm(dataset_val.ids):\n",
        "        # Get image path\n",
        "        img_info = dataset_val.coco.loadImgs(img_id)[0]\n",
        "        img_path = os.path.join(dataset_val.root, img_info['file_name'])\n",
        "\n",
        "        # Process image\n",
        "        outputs = process_image(img_path, model, postprocessors, args.device)\n",
        "\n",
        "        # Convert to COCO format\n",
        "        boxes = outputs['boxes'].tolist()\n",
        "        scores = outputs['scores'].tolist()\n",
        "        labels = outputs['labels'].tolist()\n",
        "\n",
        "        for box, score, label in zip(boxes, scores, labels):\n",
        "            xmin, ymin, xmax, ymax = box\n",
        "            width = xmax - xmin\n",
        "            height = ymax - ymin\n",
        "            results.append({\n",
        "                'image_id': img_id,\n",
        "                'category_id': label,\n",
        "                'bbox': [xmin, ymin, width, height],\n",
        "                'score': score\n",
        "            })\n",
        "\n",
        "        # Visualize detections\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        output_path = os.path.join(output_dir, f'detection_{img_id}.png')\n",
        "        visualize_detections(image, boxes, labels, scores, output_path)\n",
        "\n",
        "    # Save results for evaluation\n",
        "    result_path = os.path.join(output_dir, 'detections.json')\n",
        "    with open(result_path, 'w') as f:\n",
        "        json.dump(results, f)\n",
        "\n",
        "    # Evaluate using COCO API\n",
        "    coco_dt = coco_gt.loadRes(result_path)\n",
        "    coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()\n",
        "\n",
        "    # Save AP and AR values\n",
        "    metrics_path = os.path.join(output_dir, 'metrics.txt')\n",
        "    with open(metrics_path, 'w') as f:\n",
        "        f.write(f\"AP @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = {coco_eval.stats[0]:.3f}\\n\")\n",
        "        f.write(f\"AP @[ IoU=0.50      | area=   all | maxDets=100 ] = {coco_eval.stats[1]:.3f}\\n\")\n",
        "        f.write(f\"AP @[ IoU=0.75      | area=   all | maxDets=100 ] = {coco_eval.stats[2]:.3f}\\n\")\n",
        "        f.write(f\"AP @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = {coco_eval.stats[3]:.3f}\\n\")\n",
        "        f.write(f\"AP @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = {coco_eval.stats[4]:.3f}\\n\")\n",
        "        f.write(f\"AP @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = {coco_eval.stats[5]:.3f}\\n\")\n",
        "        f.write(f\"AR @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = {coco_eval.stats[6]:.3f}\\n\")\n",
        "        f.write(f\"AR @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = {coco_eval.stats[7]:.3f}\\n\")\n",
        "        f.write(f\"AR @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = {coco_eval.stats[8]:.3f}\\n\")\n",
        "        f.write(f\"AR @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = {coco_eval.stats[9]:.3f}\\n\")\n",
        "        f.write(f\"AR @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = {coco_eval.stats[10]:.3f}\\n\")\n",
        "        f.write(f\"AR @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = {coco_eval.stats[11]:.3f}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ_oHFMdubah",
        "outputId": "b834f945-84fa-4907-f3d7-d30b589d68de"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as T\n",
        "from util.slconfig import SLConfig\n",
        "from datasets import build_dataset\n",
        "\n",
        "# Custom collate function to resize tensors directly if they are not PIL Images\n",
        "def collate_fn(batch):\n",
        "    images, targets = zip(*batch)\n",
        "\n",
        "    # Process each image\n",
        "    processed_images = []\n",
        "    for image in images:\n",
        "        if isinstance(image, torch.Tensor):\n",
        "            # If image is already a tensor, resize using interpolation\n",
        "            image = torch.nn.functional.interpolate(image.unsqueeze(0), size=(800, 800), mode=\"bilinear\", align_corners=False).squeeze(0)\n",
        "        else:\n",
        "            # If image is a PIL Image or ndarray, use torchvision.transforms\n",
        "            transform = T.Compose([\n",
        "                T.Resize((800, 800)),  # Resize all images to 800x800\n",
        "                T.ToTensor()           # Convert images to tensor format\n",
        "            ])\n",
        "            image = transform(image)\n",
        "        processed_images.append(image)\n",
        "\n",
        "    # Stack the images into a batch (batch_size, channels, height, width)\n",
        "    images = torch.stack(processed_images, dim=0)\n",
        "\n",
        "    return images, targets\n",
        "\n",
        "# Function to build datasets and check data loading\n",
        "def build_and_check_dataset_with_collate():\n",
        "    config_file = 'DINO/config/DINO/DINO_4scale.py'\n",
        "    args = SLConfig.fromfile(config_file)\n",
        "\n",
        "    try:\n",
        "        dataset_train = build_dataset(image_set='train', args=args)\n",
        "        dataset_val = build_dataset(image_set='val', args=args)\n",
        "\n",
        "        # DataLoader with the custom collate function\n",
        "        data_loader_train = DataLoader(dataset_train, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "        data_loader_val = DataLoader(dataset_val, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "        print(f\"Train dataset length: {len(dataset_train)}\")\n",
        "        print(f\"Validation dataset length: {len(dataset_val)}\")\n",
        "\n",
        "        # Checking if data is loaded correctly\n",
        "        first_train_batch = next(iter(data_loader_train))\n",
        "        first_val_batch = next(iter(data_loader_val))\n",
        "\n",
        "        print(\"\\nFirst batch of training data:\")\n",
        "        print(f\"Images shape: {first_train_batch[0].shape}\")\n",
        "        print(f\"Targets content: {first_train_batch[1]}\")\n",
        "\n",
        "        print(\"\\nFirst batch of validation data:\")\n",
        "        print(f\"Images shape: {first_val_batch[0].shape}\")\n",
        "        print(f\"Targets content: {first_val_batch[1]}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {str(e)}\")\n",
        "        print(\"Please check your dataset paths and configuration.\")\n",
        "\n",
        "build_and_check_dataset_with_collate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NK4L3gRWxVD2",
        "outputId": "a761f805-bdda-4b4c-9a94-a11d60f80def"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms as T\n",
        "from datasets import build_dataset\n",
        "from util.slconfig import SLConfig\n",
        "\n",
        "# Custom collate function to resize images dynamically\n",
        "def collate_fn(batch):\n",
        "    images, targets = zip(*batch)\n",
        "\n",
        "    # Define a transform only for PIL images\n",
        "    transform = T.Resize((800, 800))\n",
        "\n",
        "    processed_images = []\n",
        "    for image in images:\n",
        "        if isinstance(image, torch.Tensor):\n",
        "            # If image is already a tensor, resize it using interpolation\n",
        "            image = torch.nn.functional.interpolate(image.unsqueeze(0), size=(800, 800), mode=\"bilinear\", align_corners=False).squeeze(0)\n",
        "        else:\n",
        "            # If image is a PIL Image or ndarray, apply the transform\n",
        "            image = T.ToTensor()(transform(image))\n",
        "        processed_images.append(image)\n",
        "\n",
        "    # Stack the images into a batch (batch_size, channels, height, width)\n",
        "    images = torch.stack(processed_images, dim=0)\n",
        "\n",
        "    return images, targets\n",
        "\n",
        "# Step 2: Verify dataset loading and targets after resizing\n",
        "def check_dataset_and_targets():\n",
        "    # Load the configuration and dataset\n",
        "    config_file = 'DINO/config/DINO/DINO_4scale.py'\n",
        "    args = SLConfig.fromfile(config_file)\n",
        "\n",
        "    dataset_train = build_dataset(image_set='train', args=args)\n",
        "\n",
        "    # Use a DataLoader with the updated collate_fn\n",
        "    data_loader_train = DataLoader(dataset_train, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    # Fetch the first batch of data\n",
        "    images, targets = next(iter(data_loader_train))\n",
        "\n",
        "    print(f\"Number of images in batch: {len(images)}\")\n",
        "    print(f\"Image shape: {images.shape}\")\n",
        "    print(f\"Number of targets: {len(targets)}\")\n",
        "\n",
        "    # Check the targets for each image\n",
        "    for i, target in enumerate(targets):\n",
        "        print(f\"Image {i}:\")\n",
        "        print(f\"  Labels: {target.get('labels', 'No labels found')}\")\n",
        "        print(f\"  Boxes: {target.get('boxes', 'No boxes found')}\")\n",
        "\n",
        "check_dataset_and_targets()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4MVzB206Mqm",
        "outputId": "83c37047-0909-4cf3-8706-208f52a35d4d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "from datasets import build_dataset\n",
        "from util.slconfig import SLConfig\n",
        "\n",
        "# Custom collate function to gather information about the image sizes\n",
        "def collate_fn(batch):\n",
        "    images, targets = zip(*batch)\n",
        "\n",
        "    # Define the resize transformation\n",
        "    transform = T.Compose([\n",
        "        T.Resize((800, 800)),  # Force resize to 800x800\n",
        "        T.ToTensor()           # Convert to tensor\n",
        "    ])\n",
        "\n",
        "    processed_images = []\n",
        "\n",
        "    # Process each image\n",
        "    for i, image in enumerate(images):\n",
        "        if isinstance(image, Image.Image):\n",
        "            print(f\"Image {i}: Original size: {image.size}\")\n",
        "            resized_image = transform(image)\n",
        "            processed_images.append(resized_image)\n",
        "            print(f\"Image {i}: Resized to: {resized_image.shape}\")\n",
        "        else:\n",
        "            print(f\"Image {i}: Not a PIL image (already a tensor)\")\n",
        "            processed_images.append(image)\n",
        "\n",
        "    # Attempt to stack the images into a batch and print the result\n",
        "    try:\n",
        "        images = torch.stack(processed_images, dim=0)\n",
        "        print(f\"Stacked images shape: {images.shape}\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Error when stacking images: {e}\")\n",
        "\n",
        "    return processed_images, targets\n",
        "\n",
        "# Step 1: Load dataset and gather information about image sizes and targets\n",
        "def gather_image_info():\n",
        "    # Load configuration and dataset\n",
        "    config_file = 'DINO/config/DINO/DINO_4scale.py'\n",
        "    args = SLConfig.fromfile(config_file)\n",
        "\n",
        "    dataset_train = build_dataset(image_set='train', args=args)\n",
        "    data_loader_train = DataLoader(dataset_train, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    # Print the sizes of images in the first batch and inspect targets\n",
        "    for images, targets in data_loader_train:\n",
        "        print(f\"\\nNumber of images in batch: {len(images)}\")\n",
        "\n",
        "        # Check target structure\n",
        "        for i, target in enumerate(targets):\n",
        "            print(f\"\\nTarget {i}:\")\n",
        "            if target is None:\n",
        "                print(\"Target is None\")\n",
        "            else:\n",
        "                print(f\"  Labels: {target.get('labels', 'No labels found')}\")\n",
        "                print(f\"  Boxes: {target.get('boxes', 'No boxes found')}\")\n",
        "                print(f\"  Other info: {target.keys()}\")\n",
        "\n",
        "        # Exit after first batch to prevent overwhelming output\n",
        "        break\n",
        "\n",
        "# Step 2: Gather the information by running the function\n",
        "gather_image_info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXbYdhKi6pqz",
        "outputId": "6da28955-aa38-4a4e-bc3e-f943124f63bf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import build_dataset\n",
        "from util.slconfig import SLConfig\n",
        "\n",
        "# Custom collate function to resize tensor images and stack them\n",
        "def collate_fn(batch):\n",
        "    images, targets = zip(*batch)\n",
        "\n",
        "    # Define the target size for resizing (800x800)\n",
        "    target_size = (800, 800)\n",
        "\n",
        "    processed_images = []\n",
        "\n",
        "    # Process each image\n",
        "    for i, image in enumerate(images):\n",
        "        if isinstance(image, torch.Tensor):\n",
        "            print(f\"Image {i}: Original size: {image.shape}\")\n",
        "            # Resize tensor image to (800, 800) using interpolate\n",
        "            resized_image = F.interpolate(image.unsqueeze(0), size=target_size, mode='bilinear', align_corners=False).squeeze(0)\n",
        "            processed_images.append(resized_image)\n",
        "            print(f\"Image {i}: Resized to: {resized_image.shape}\")\n",
        "        else:\n",
        "            print(f\"Image {i}: Unexpected data type\")\n",
        "            processed_images.append(image)\n",
        "\n",
        "    # Stack the images into a batch (batch_size, channels, height, width)\n",
        "    try:\n",
        "        images = torch.stack(processed_images, dim=0)\n",
        "        print(f\"Stacked images shape: {images.shape}\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Error when stacking images: {e}\")\n",
        "\n",
        "    return images, targets\n",
        "\n",
        "# Step 1: Load dataset and gather information about image sizes and targets\n",
        "def gather_image_info():\n",
        "    # Load configuration and dataset\n",
        "    config_file = 'DINO/config/DINO/DINO_4scale.py'\n",
        "    args = SLConfig.fromfile(config_file)\n",
        "\n",
        "    dataset_train = build_dataset(image_set='train', args=args)\n",
        "    data_loader_train = DataLoader(dataset_train, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    # Print the sizes of images in the first batch and inspect targets\n",
        "    for images, targets in data_loader_train:\n",
        "        print(f\"\\nNumber of images in batch: {len(images)}\")\n",
        "\n",
        "        # Check target structure\n",
        "        for i, target in enumerate(targets):\n",
        "            print(f\"\\nTarget {i}:\")\n",
        "            if target is None:\n",
        "                print(\"Target is None\")\n",
        "            else:\n",
        "                print(f\"  Labels: {target.get('labels', 'No labels found')}\")\n",
        "                print(f\"  Boxes: {target.get('boxes', 'No boxes found')}\")\n",
        "                print(f\"  Other info: {target.keys()}\")\n",
        "\n",
        "        # Exit after first batch to prevent overwhelming output\n",
        "        break\n",
        "\n",
        "# Step 2: Gather the information by running the function\n",
        "gather_image_info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLxGfskN8YHK",
        "outputId": "5e3068e8-8eea-4da0-eb94-083eb6e02ba0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import build_dataset\n",
        "from util.slconfig import SLConfig\n",
        "\n",
        "# Custom collate function to resize tensor images and perform checks\n",
        "def collate_fn(batch):\n",
        "    images, targets = zip(*batch)\n",
        "\n",
        "    # Define the target size for resizing (800x800)\n",
        "    target_size = (800, 800)\n",
        "\n",
        "    processed_images = []\n",
        "\n",
        "    # Process each image\n",
        "    for image in images:\n",
        "        if isinstance(image, torch.Tensor):\n",
        "            resized_image = F.interpolate(image.unsqueeze(0), size=target_size, mode='bilinear', align_corners=False).squeeze(0)\n",
        "            processed_images.append(resized_image)\n",
        "        else:\n",
        "            print(\"Unexpected image type, expected tensor.\")\n",
        "\n",
        "    images = torch.stack(processed_images, dim=0)\n",
        "\n",
        "    return images, targets\n",
        "\n",
        "# Step 1: Load dataset and dataloader to perform pre-checks\n",
        "def run_pre_checks():\n",
        "    # Load the configuration and dataset\n",
        "    config_file = 'DINO/config/DINO/DINO_4scale.py'\n",
        "    args = SLConfig.fromfile(config_file)\n",
        "\n",
        "    dataset_train = build_dataset(image_set='train', args=args)\n",
        "    data_loader_train = DataLoader(dataset_train, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    # Counters for NoneType targets and total batches\n",
        "    none_type_count = 0\n",
        "    batch_count = 0\n",
        "\n",
        "    # Iterate over the dataset and perform checks\n",
        "    for batch_idx, (images, targets) in enumerate(data_loader_train):\n",
        "        batch_count += 1\n",
        "        print(f\"\\nBatch {batch_idx + 1} - Number of images: {len(images)}\")\n",
        "\n",
        "        # Check for NoneType targets and print structure\n",
        "        for target_idx, target in enumerate(targets):\n",
        "            if target is None:\n",
        "                print(f\"Target {target_idx} is None.\")\n",
        "                none_type_count += 1\n",
        "            else:\n",
        "                print(f\"Target {target_idx}: Labels: {target.get('labels', 'No labels found')}, Boxes: {target.get('boxes', 'No boxes found')}\")\n",
        "\n",
        "        # Break after checking a few batches to avoid overwhelming output\n",
        "        if batch_idx >= 10:\n",
        "            break\n",
        "\n",
        "    print(f\"\\nTotal batches checked: {batch_count}\")\n",
        "    print(f\"Total NoneType targets found: {none_type_count}\")\n",
        "\n",
        "# Step 2: Run the pre-checks\n",
        "run_pre_checks()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Qunp6vCXAGbG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def debug_forward_pass(model, images, targets):\n",
        "    # Print image and target information\n",
        "    print(f\"Image shapes: {[img.shape for img in images]}\")\n",
        "    print(f\"Targets type: {type(targets)}\")\n",
        "\n",
        "    # Log each target\n",
        "    for idx, target in enumerate(targets):\n",
        "        if target is None:\n",
        "            print(f\"Target {idx} is None\")\n",
        "        else:\n",
        "            print(f\"Target {idx}: Labels: {target.get('labels', 'No labels')} Boxes: {target.get('boxes', 'No boxes')}\")\n",
        "\n",
        "    # Simulate model forward pass\n",
        "    if model.dn_number > 0 or targets is not None:\n",
        "        try:\n",
        "            # Log the preparation for CDN to understand the flow\n",
        "            print(\"Preparing for CDN...\")\n",
        "            input_query_label, input_query_bbox, attn_mask, dn_meta = prepare_for_cdn(\n",
        "                dn_args=(targets, model.dn_number, model.dn_label_noise_ratio, model.dn_box_noise_scale),\n",
        "                training=model.training,\n",
        "                num_queries=model.num_queries,\n",
        "                num_classes=model.num_classes,\n",
        "                hidden_dim=model.hidden_dim,\n",
        "                label_enc=model.label_enc\n",
        "            )\n",
        "            print(f\"CDN preparation successful. dn_meta: {dn_meta}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error preparing for CDN: {e}\")\n",
        "            return\n",
        "\n",
        "    # Check if model can process the images\n",
        "    try:\n",
        "        print(\"Passing images through model...\")\n",
        "        outputs = model(images)\n",
        "        print(\"Model forward pass successful.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during forward pass: {e}\")\n",
        "\n",
        "    return outputs\n",
        "\n",
        "# Use this function in your training loop:\n",
        "# for images, targets in data_loader:\n",
        "#     outputs = debug_forward_pass(model, images, targets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Pc_5lNt1IS1p",
        "outputId": "caf4b217-8833-4c03-dfdd-55cb0ef73e1b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Add DINO directory to system path\n",
        "dino_path = \"DINO\"\n",
        "sys.path.append(dino_path)\n",
        "\n",
        "from util.slconfig import SLConfig\n",
        "from datasets import build_dataset\n",
        "from models import build_model\n",
        "\n",
        "def debug_script():\n",
        "    print(\"1. Loading configuration\")\n",
        "    config_file = 'DINO/config/DINO/DINO_4scale.py'\n",
        "    args = SLConfig.fromfile(config_file)\n",
        "    args.pretrain_model_path = 'DINO-4scale model/checkpoint0033_4scale.pth'\n",
        "    args.output_dir = 'DINO-4scale model/DINO-4scale_Fine-tune'\n",
        "    args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    print(\"\\n2. Building dataset\")\n",
        "    dataset_train = build_dataset(image_set='train', args=args)\n",
        "    print(f\"Dataset size: {len(dataset_train)}\")\n",
        "\n",
        "    print(\"\\n3. Checking sample item\")\n",
        "    sample_item = dataset_train[0]\n",
        "    print(f\"Sample item type: {type(sample_item)}\")\n",
        "    if isinstance(sample_item, tuple):\n",
        "        print(f\"Sample item length: {len(sample_item)}\")\n",
        "        print(f\"Sample image shape: {sample_item[0].shape}\")\n",
        "        print(f\"Sample target: {sample_item[1]}\")\n",
        "\n",
        "    print(\"\\n4. Building model\")\n",
        "    try:\n",
        "        model, criterion, postprocessors = build_model(args)\n",
        "        print(f\"Model structure:\\n{model}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error building model: {str(e)}\")\n",
        "        print(\"Model configuration:\")\n",
        "        for key, value in args.items():\n",
        "            print(f\"{key}: {value}\")\n",
        "\n",
        "    print(\"\\n5. Checking prepare_for_cdn function\")\n",
        "    utils_path = os.path.join(dino_path, \"models\", \"dino\", \"utils.py\")\n",
        "    if os.path.exists(utils_path):\n",
        "        with open(utils_path, 'r') as f:\n",
        "            content = f.read()\n",
        "            print(\"Content of utils.py:\")\n",
        "            print(content)\n",
        "    else:\n",
        "        print(f\"utils.py not found at {utils_path}\")\n",
        "\n",
        "debug_script()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Uzrip_ZdR8Hj",
        "outputId": "57c5dbc2-4c28-4247-8ecb-4743478120cd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "dino_path = \"DINO\"\n",
        "sys.path.append(dino_path)\n",
        "\n",
        "from util.slconfig import SLConfig\n",
        "from datasets import build_dataset\n",
        "from models import build_model\n",
        "from util.misc import nested_tensor_from_tensor_list\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images = [item[0] for item in batch]\n",
        "    targets = [item[1] for item in batch]\n",
        "    batched_imgs = nested_tensor_from_tensor_list(images)\n",
        "    return batched_imgs, targets\n",
        "\n",
        "def examine_dataset(dataset, num_samples=5):\n",
        "    print(f\"Dataset type: {type(dataset)}\")\n",
        "    print(f\"Dataset length: {len(dataset)}\")\n",
        "    for i in range(min(num_samples, len(dataset))):\n",
        "        sample = dataset[i]\n",
        "        print(f\"\\nSample {i}:\")\n",
        "        print(f\"  Image shape: {sample[0].shape}\")\n",
        "        print(f\"  Target keys: {sample[1].keys()}\")\n",
        "        print(f\"  Target 'labels' shape: {sample[1]['labels'].shape}\")\n",
        "        print(f\"  Target 'boxes' shape: {sample[1]['boxes'].shape}\")\n",
        "\n",
        "def examine_model(model):\n",
        "    print(f\"Model type: {type(model)}\")\n",
        "    print(\"Model parameters:\")\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"prepare_for_cdn\" in name:\n",
        "            print(f\"  {name}: {param.shape}\")\n",
        "\n",
        "def examine_config(args):\n",
        "    print(\"Configuration parameters:\")\n",
        "    for key, value in args.items():\n",
        "        if key in ['dn_number', 'dn_label_noise_ratio', 'dn_box_noise_scale', 'use_dn']:\n",
        "            print(f\"  {key}: {value}\")\n",
        "\n",
        "def main():\n",
        "    # Load configuration\n",
        "    config_file = 'DINO/config/DINO/DINO_4scale.py'\n",
        "    args = SLConfig.fromfile(config_file)\n",
        "    args.pretrain_model_path = 'DINO-4scale model/checkpoint0033_4scale.pth'\n",
        "    args.output_dir = 'DINO-4scale model/DINO-4scale_Fine-tune'\n",
        "    args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    print(\"\\n1. Examining dataset:\")\n",
        "    dataset_train = build_dataset(image_set='train', args=args)\n",
        "    examine_dataset(dataset_train)\n",
        "\n",
        "    print(\"\\n2. Examining dataloader:\")\n",
        "    data_loader_train = DataLoader(dataset_train, batch_size=2, shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
        "    for samples, targets in data_loader_train:\n",
        "        print(f\"Batch samples shape: {samples.tensors.shape}\")\n",
        "        print(f\"Batch targets length: {len(targets)}\")\n",
        "        print(f\"First target in batch: {targets[0].keys()}\")\n",
        "        break\n",
        "\n",
        "    print(\"\\n3. Examining model:\")\n",
        "    model, criterion, _ = build_model(args)\n",
        "    examine_model(model)\n",
        "\n",
        "    print(\"\\n4. Examining configuration:\")\n",
        "    examine_config(args)\n",
        "\n",
        "    print(\"\\n5. Checking prepare_for_cdn function:\")\n",
        "    dn_components_path = os.path.join(dino_path, \"models\", \"dino\", \"dn_components.py\")\n",
        "    if os.path.exists(dn_components_path):\n",
        "        with open(dn_components_path, 'r') as f:\n",
        "            print(f.read())\n",
        "    else:\n",
        "        print(f\"dn_components.py not found at {dn_components_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yKECaW-cgeX1",
        "outputId": "28ece59b-de33-45fa-899c-abdc98677db9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "# Add DINO directory to system path\n",
        "dino_path = \"DINO\"\n",
        "sys.path.append(dino_path)\n",
        "\n",
        "from util.slconfig import SLConfig\n",
        "from datasets import build_dataset\n",
        "from models import build_dino\n",
        "from util.misc import NestedTensor, nested_tensor_from_tensor_list\n",
        "import models.dino.dn_components as dn_components\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = list(zip(*batch))\n",
        "    batch[0] = nested_tensor_from_tensor_list(batch[0])\n",
        "    return tuple(batch)\n",
        "\n",
        "def modified_prepare_for_cdn(dn_args, training, num_queries, num_classes, hidden_dim, label_enc):\n",
        "    if training and dn_args[0] is not None:\n",
        "        return dn_components.original_prepare_for_cdn(dn_args, training, num_queries, num_classes, hidden_dim, label_enc)\n",
        "    return None, None, None, None\n",
        "\n",
        "dn_components.original_prepare_for_cdn = dn_components.prepare_for_cdn\n",
        "dn_components.prepare_for_cdn = modified_prepare_for_cdn\n",
        "\n",
        "def calculate_accuracy(loss_ce):\n",
        "    return max(0, 100 - loss_ce * 10)  # Scale it to a percentage\n",
        "\n",
        "def train_one_epoch(model, criterion, data_loader, optimizer, device, epoch):\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "\n",
        "    metric_logger = defaultdict(float)\n",
        "    num_batches = len(data_loader)\n",
        "\n",
        "    with tqdm(total=num_batches, desc=f\"Epoch {epoch+1}\") as pbar:\n",
        "        for samples, targets in data_loader:\n",
        "            samples = samples.to(device)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            outputs = model(samples, targets)\n",
        "            loss_dict = criterion(outputs, targets)\n",
        "            weight_dict = criterion.weight_dict\n",
        "            losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            for k, v in loss_dict.items():\n",
        "                metric_logger[k] += v.item()\n",
        "            metric_logger['total_loss'] += losses.item()\n",
        "\n",
        "            accuracy = calculate_accuracy(loss_dict['loss_ce'].item())\n",
        "            metric_logger['accuracy'] += accuracy\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix({\n",
        "                \"Loss\": f\"{metric_logger['total_loss'] / (pbar.n + 1):.2f}\",\n",
        "                \"Acc\": f\"{metric_logger['accuracy'] / (pbar.n + 1):.2f}%\"\n",
        "            })\n",
        "\n",
        "    metric_logger = {k: v / num_batches for k, v in metric_logger.items()}\n",
        "    return metric_logger\n",
        "\n",
        "def main():\n",
        "    config_file = 'DINO/config/DINO/DINO_4scale.py'\n",
        "    args = SLConfig.fromfile(config_file)\n",
        "\n",
        "    args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    args.lr = 1e-5\n",
        "    args.lr_backbone = 1e-6\n",
        "    args.batch_size = 2\n",
        "    args.epochs = 100\n",
        "    args.lr_drop = 4\n",
        "    args.output_dir = \"DINO-4scale model/DINO-4scale_Fine-tune\"\n",
        "\n",
        "    dataset_train = build_dataset(image_set='train', args=args)\n",
        "    data_loader_train = DataLoader(dataset_train, batch_size=args.batch_size, shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
        "\n",
        "    model, criterion, postprocessors = build_dino(args)\n",
        "    model.to(args.device)\n",
        "\n",
        "    checkpoint = torch.load(args.pretrain_model_path, map_location=args.device)\n",
        "    model.load_state_dict(checkpoint['model'], strict=False)\n",
        "\n",
        "    param_dicts = [\n",
        "        {\"params\": [p for n, p in model.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
        "            \"lr\": args.lr_backbone,\n",
        "        },\n",
        "    ]\n",
        "    optimizer = optim.AdamW(param_dicts, lr=args.lr, weight_decay=args.weight_decay)\n",
        "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_drop)\n",
        "\n",
        "    num_epochs = args.epochs\n",
        "    metrics_history = defaultdict(list)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_metrics = train_one_epoch(model, criterion, data_loader_train, optimizer, args.device, epoch)\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        for k, v in epoch_metrics.items():\n",
        "            metrics_history[k].append(v)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1} summary:\")\n",
        "        print(f\"  Loss: {epoch_metrics['total_loss']:.4f}\")\n",
        "        print(f\"  Accuracy: {epoch_metrics['accuracy']:.2f}%\")\n",
        "\n",
        "        checkpoint_path = os.path.join(args.output_dir, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
        "        torch.save({\n",
        "            'model': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'lr_scheduler': lr_scheduler.state_dict(),\n",
        "            'epoch': epoch,\n",
        "            'args': args,\n",
        "        }, checkpoint_path)\n",
        "\n",
        "    # Save final fine-tuned model\n",
        "    final_model_path = os.path.join(args.output_dir, \"final_model.pth\")\n",
        "    torch.save(model.state_dict(), final_model_path)\n",
        "    print(f\"\\nFinal fine-tuned model saved to: {final_model_path}\")\n",
        "\n",
        "    # Plot metrics\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(range(1, num_epochs+1), metrics_history['total_loss'], label='Loss')\n",
        "    plt.plot(range(1, num_epochs+1), metrics_history['accuracy'], label='Accuracy')\n",
        "    plt.title('Training Metrics over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Value')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(args.output_dir, 'metrics_plot.png'))\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Fine-tuning completed. Model and metrics saved to:\", args.output_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqhWbZPPvKSn",
        "outputId": "174a56b1-0066-4e32-c7d1-fc483f096f25"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Add DINO directory to sys.path\n",
        "sys.path.append('DINO')\n",
        "\n",
        "from util.slconfig import SLConfig\n",
        "from models import build_model\n",
        "from datasets import build_dataset\n",
        "from util.misc import nested_tensor_from_tensor_list\n",
        "from util.box_ops import box_cxcywh_to_xyxy\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "\n",
        "def load_model(args):\n",
        "    model, _, postprocessors = build_model(args)\n",
        "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
        "    if isinstance(checkpoint, dict) and 'model' in checkpoint:\n",
        "        model.load_state_dict(checkpoint['model'])\n",
        "    else:\n",
        "        model.load_state_dict(checkpoint)\n",
        "    model.eval()\n",
        "    return model, postprocessors\n",
        "\n",
        "def process_image(image_path, model, postprocessors, device):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    w, h = image.size\n",
        "    img = torch.from_numpy(np.array(image)).permute(2, 0, 1).float()\n",
        "    img = nested_tensor_from_tensor_list([img]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "\n",
        "    results = postprocessors['bbox'](outputs, torch.tensor([[w, h]]).to(device))[0]\n",
        "    return results\n",
        "\n",
        "def visualize_detections(image, boxes, labels, scores, output_path):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.imshow(image)\n",
        "    ax = plt.gca()\n",
        "\n",
        "    for box, label, score in zip(boxes, labels, scores):\n",
        "        x1, y1, x2, y2 = box\n",
        "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='red', linewidth=2)\n",
        "        ax.add_patch(rect)\n",
        "        plt.text(x1, y1, f'{label}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n",
        "    plt.close()\n",
        "\n",
        "def main():\n",
        "    # Load configuration\n",
        "    config_file = 'DINO/config/DINO/DINO_4scale.py'\n",
        "    args = SLConfig.fromfile(config_file)\n",
        "    args.device = 'cuda'\n",
        "    args.resume = 'DINO-4scale model/DINO-4scale_Fine-tune/final_model.pth'\n",
        "\n",
        "    # Create output directory\n",
        "    output_dir = 'output path'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        # Load model\n",
        "        model, postprocessors = load_model(args)\n",
        "        model.to(args.device)\n",
        "\n",
        "        # Load validation dataset\n",
        "        args.coco_path = 'place holder'\n",
        "        args.eval = True\n",
        "        dataset_val = build_dataset(image_set='val', args=args)\n",
        "\n",
        "        # Initialize COCO API\n",
        "        ann_file = os.path.join(args.coco_path, 'val', 'val.json')\n",
        "        coco_gt = COCO(ann_file)\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for img_id in tqdm(dataset_val.ids):\n",
        "            # Get image path\n",
        "            img_info = dataset_val.coco.loadImgs(img_id)[0]\n",
        "            img_path = os.path.join(dataset_val.root, img_info['file_name'])\n",
        "\n",
        "            # Process image\n",
        "            outputs = process_image(img_path, model, postprocessors, args.device)\n",
        "\n",
        "            # Convert to COCO format\n",
        "            boxes = outputs['boxes'].tolist()\n",
        "            scores = outputs['scores'].tolist()\n",
        "            labels = outputs['labels'].tolist()\n",
        "\n",
        "            for box, score, label in zip(boxes, scores, labels):\n",
        "                xmin, ymin, xmax, ymax = box\n",
        "                width = xmax - xmin\n",
        "                height = ymax - ymin\n",
        "                results.append({\n",
        "                    'image_id': img_id,\n",
        "                    'category_id': label,\n",
        "                    'bbox': [xmin, ymin, width, height],\n",
        "                    'score': score\n",
        "                })\n",
        "\n",
        "            # Visualize detections\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            output_path = os.path.join(output_dir, f'detection_{img_id}.png')\n",
        "            visualize_detections(image, boxes, labels, scores, output_path)\n",
        "\n",
        "        # Save results for evaluation\n",
        "        result_path = os.path.join(output_dir, 'detections.json')\n",
        "        with open(result_path, 'w') as f:\n",
        "            json.dump(results, f)\n",
        "\n",
        "        # Evaluate using COCO API\n",
        "        coco_dt = coco_gt.loadRes(result_path)\n",
        "        coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
        "        coco_eval.evaluate()\n",
        "        coco_eval.accumulate()\n",
        "        coco_eval.summarize()\n",
        "\n",
        "        # Save AP and AR values\n",
        "        metrics_path = os.path.join(output_dir, 'metrics.txt')\n",
        "        with open(metrics_path, 'w') as f:\n",
        "            f.write(f\"AP @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = {coco_eval.stats[0]:.3f}\\n\")\n",
        "            f.write(f\"AP @[ IoU=0.50      | area=   all | maxDets=100 ] = {coco_eval.stats[1]:.3f}\\n\")\n",
        "            f.write(f\"AP @[ IoU=0.75      | area=   all | maxDets=100 ] = {coco_eval.stats[2]:.3f}\\n\")\n",
        "            f.write(f\"AP @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = {coco_eval.stats[3]:.3f}\\n\")\n",
        "            f.write(f\"AP @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = {coco_eval.stats[4]:.3f}\\n\")\n",
        "            f.write(f\"AP @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = {coco_eval.stats[5]:.3f}\\n\")\n",
        "            f.write(f\"AR @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = {coco_eval.stats[6]:.3f}\\n\")\n",
        "            f.write(f\"AR @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = {coco_eval.stats[7]:.3f}\\n\")\n",
        "            f.write(f\"AR @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = {coco_eval.stats[8]:.3f}\\n\")\n",
        "            f.write(f\"AR @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = {coco_eval.stats[9]:.3f}\\n\")\n",
        "            f.write(f\"AR @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = {coco_eval.stats[10]:.3f}\\n\")\n",
        "            f.write(f\"AR @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = {coco_eval.stats[11]:.3f}\\n\")\n",
        "\n",
        "        print(f\"Evaluation complete. Results saved in {output_dir}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
